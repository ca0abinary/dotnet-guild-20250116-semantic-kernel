{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# üîé Vector Search\n",
    "\n",
    "Vector search is a technique used in information retrieval where data is represented as vectors in a high-dimensional space. These vectors are typically dense representations of the data, capturing both semantic and syntactic features.\n",
    "\n",
    "In this example we will build a small vector database that enables natural language lookup of terms. A vector search is not the same as an LLM query; it's faster and more focused, but cannot capture nuance or generate content. Both techniques have tradeoffs but can be used together to build powerful information systems.\n",
    "\n",
    "## üõ†Ô∏è Setup\n",
    "\n",
    "We will start by installing the packages we need and creating a connection to Ollama's embeddings model.\n",
    "> *Note:* This is not an LLM, but rather a much smaller model for returning vectors based on input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.Extensions.AI.Ollama, 9.0.1-preview.1.24570.5</span></li><li><span>Microsoft.SemanticKernel.Connectors.InMemory, 1.33.0-preview</span></li><li><span>Microsoft.SemanticKernel.Connectors.Ollama, 1.32.0-alpha</span></li><li><span>OllamaSharp, 5.0.2</span></li><li><span>System.Linq.Async, 6.0.1</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: System.Linq.Async\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Connectors.Ollama, 1.32.0-alpha\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Connectors.InMemory, 1.33.0-preview\"\n",
    "#r \"nuget: Microsoft.Extensions.AI.Ollama, 9.0.1-preview.1.24570.5\"\n",
    "#r \"nuget: OllamaSharp\"\n",
    "\n",
    "using Microsoft.Extensions.VectorData;\n",
    "using Microsoft.SemanticKernel.Connectors.InMemory;\n",
    "using Microsoft.SemanticKernel.Embeddings;\n",
    "using OllamaSharp;\n",
    "\n",
    "using System.Net.Sockets;\n",
    "\n",
    "var ollamaApiUrl = \"http://localhost:11434\";\n",
    "try {\n",
    "  var tcp = new TcpClient();\n",
    "  tcp.SendTimeout = 100;\n",
    "  tcp.Connect(\"host.docker.internal\", 11434);\n",
    "  ollamaApiUrl = \"host.docker.internal\";\n",
    "} catch { }\n",
    "\n",
    "var embeddingsModel = \"nomic-embed-text\";\n",
    "\n",
    "// Create an embedding generation service.\n",
    "var client = new OllamaApiClient(ollamaApiUrl, embeddingsModel);\n",
    "var textEmbeddingGenerationService = client.AsEmbeddingGenerationService();\n",
    "\n",
    "// Construct an InMemory vector store.\n",
    "var vectorStore = new InMemoryVectorStore();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìí Define our glossary\n",
    "\n",
    "We need to create an annotated object to let the vector database know how our objects look and which items need to be indexed for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private sealed class Glossary\n",
    "{\n",
    "    [VectorStoreRecordKey]\n",
    "    public ulong Key { get; set; }\n",
    "\n",
    "    [VectorStoreRecordData(IsFilterable = true)]\n",
    "    public string Category { get; set; } = string.Empty;\n",
    "\n",
    "    [VectorStoreRecordData]\n",
    "    public string Term { get; set; } = string.Empty;\n",
    "\n",
    "    [VectorStoreRecordData]\n",
    "    public string Definition { get; set; } = string.Empty;\n",
    "\n",
    "    [VectorStoreRecordVector(1536)]\n",
    "    public ReadOnlyMemory<float> DefinitionEmbedding { get; set; }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöõ Create glossary definitions\n",
    "\n",
    "Now we will create the list of glossary entries. While this is a hardcoded list, it could just as easily be data retrieved from a conventional database, scraped from websites, gathered from files, or really any other source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private static IEnumerable<Glossary> CreateGlossaryEntries()\n",
    "{\n",
    "    yield return new Glossary\n",
    "    {\n",
    "        Key = 1,\n",
    "        Category = \"External Definitions\",\n",
    "        Term = \"API\",\n",
    "        Definition = \"Application Programming Interface. A set of rules and specifications that allow software components to communicate and exchange data.\"\n",
    "    };\n",
    "\n",
    "    yield return new Glossary\n",
    "    {\n",
    "        Key = 2,\n",
    "        Category = \"Core Definitions\",\n",
    "        Term = \"Connectors\",\n",
    "        Definition = \"Connectors allow you to integrate with various services provide AI capabilities, including LLM, AudioToText, TextToAudio, Embedding generation, etc.\"\n",
    "    };\n",
    "\n",
    "    yield return new Glossary\n",
    "    {\n",
    "        Key = 3,\n",
    "        Category = \"External Definitions\",\n",
    "        Term = \"RAG\",\n",
    "        Definition = \"Retrieval Augmented Generation - a term that refers to the process of retrieving additional data to provide as context to an LLM to use when generating a response (completion) to a user‚Äôs question (prompt).\"\n",
    "    };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Load the data\n",
    "\n",
    "Now we will call out to the model service (in our case Ollama) to have it generate encodings. Semantic Kernel handles most of the \"heavy lifting\" in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get and create collection if it doesn't exist.\n",
    "var collection = vectorStore.GetCollection<ulong, Glossary>(\"skglossary\");\n",
    "await collection.CreateCollectionIfNotExistsAsync();\n",
    "\n",
    "// Create glossary entries and generate embeddings for them.\n",
    "var glossaryEntries = CreateGlossaryEntries().ToList();\n",
    "var tasks = glossaryEntries.Select(entry => Task.Run(async () =>\n",
    "{\n",
    "    entry.DefinitionEmbedding = await textEmbeddingGenerationService.GenerateEmbeddingAsync(entry.Definition);\n",
    "}));\n",
    "await Task.WhenAll(tasks);\n",
    "\n",
    "// Upsert the glossary entries into the collection and return their keys.\n",
    "var upsertedKeysTasks = glossaryEntries.Select(x => collection.UpsertAsync(x));\n",
    "await Task.WhenAll(upsertedKeysTasks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Search the data\n",
    "\n",
    "Finally we have made it through all the steps to create a vector database.\n",
    "1. We defined our type\n",
    "2. Created some data\n",
    "3. Loaded that data into the vector data store\n",
    "\n",
    "Now let's see the power of a vector store in action by running a query against it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search string: What is an api?\n",
      "Result:\n",
      "          Id: 1\n",
      "        Term: API\n",
      "  Definition: Application Programming Interface. A set of rules and specifications that allow software components to communicate and exchange data.\n"
     ]
    }
   ],
   "source": [
    "// Search the collection using a vector search.\n",
    "var searchString = \"What is an api?\";\n",
    "var searchVector = await textEmbeddingGenerationService.GenerateEmbeddingAsync(searchString);\n",
    "var searchResult = await collection.VectorizedSearchAsync(searchVector, new() { Top = 1 });\n",
    "var resultRecords = await searchResult.Results.ToListAsync();\n",
    "\n",
    "Console.WriteLine(@$\"\n",
    "Search string: {searchString}\n",
    "Result:\n",
    "          Id: {resultRecords.First().Record.Key}\n",
    "        Term: {resultRecords.First().Record.Term}\n",
    "  Definition: {resultRecords.First().Record.Definition}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was a lot faster than querying a large language model! üèéÔ∏è\n",
    "We got back exactly the glossary record we wanted. Now you can start to see how classifying information and feeding it into a vector store can be helpful. We can use vector query results both pre and post LLM queries to fine tune either the initial query or the results.\n",
    "\n",
    "## üìö Read More\n",
    "\n",
    "[Optimizing Retrieval for RAG Apps: Vector Search and Hybrid Techniques](https://techcommunity.microsoft.com/blog/educatordeveloperblog/optimizing-retrieval-for-rag-apps-vector-search-and-hybrid-techniques/4138030)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
